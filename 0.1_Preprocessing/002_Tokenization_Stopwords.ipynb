{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saks0106/DL_Frequent_Lookout/blob/main/1_NLP_Tokenization_Stopwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLP BASICS"
      ],
      "metadata": {
        "id": "RHGVVOZOyROD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tokenization**"
      ],
      "metadata": {
        "id": "sT4id5pE54vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document.**\n",
        "\n",
        "We’ll be using a text narrated by Steve Jobs in the “Think Different” Apple commercial."
      ],
      "metadata": {
        "id": "SqBFpQNd6QZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjbNGAhW5i2n"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Here’s to the crazy ones, the misfits, the rebels, the troublemakers, \n",
        "the round pegs in the square holes. The ones who see things differently — they’re not fond of \n",
        "rules. You can quote them, disagree with them, glorify\n",
        "or vilify them, but the only thing you can’t do is ignore them because they\n",
        "change things. They push the human race forward, and while some may see them\n",
        "as the crazy ones, we see genius, because the ones who are crazy enough to think\n",
        "that they can change the world, are the ones who do.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Simple tokenization with .split"
      ],
      "metadata": {
        "id": "C18LOIYs6ISJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# word tokenization\n",
        "text.split()\n",
        "# sentence tokenizer\n",
        "# text.split('.') #splitting sentence by sentence"
      ],
      "metadata": {
        "id": "zXrWItKH584F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac03986-660d-4b44-c159-7b48bb69fbab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here’s',\n",
              " 'to',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones,',\n",
              " 'the',\n",
              " 'misfits,',\n",
              " 'the',\n",
              " 'rebels,',\n",
              " 'the',\n",
              " 'troublemakers,',\n",
              " 'the',\n",
              " 'round',\n",
              " 'pegs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'square',\n",
              " 'holes.',\n",
              " 'The',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'see',\n",
              " 'things',\n",
              " 'differently',\n",
              " '—',\n",
              " 'they’re',\n",
              " 'not',\n",
              " 'fond',\n",
              " 'of',\n",
              " 'rules.',\n",
              " 'You',\n",
              " 'can',\n",
              " 'quote',\n",
              " 'them,',\n",
              " 'disagree',\n",
              " 'with',\n",
              " 'them,',\n",
              " 'glorify',\n",
              " 'or',\n",
              " 'vilify',\n",
              " 'them,',\n",
              " 'but',\n",
              " 'the',\n",
              " 'only',\n",
              " 'thing',\n",
              " 'you',\n",
              " 'can’t',\n",
              " 'do',\n",
              " 'is',\n",
              " 'ignore',\n",
              " 'them',\n",
              " 'because',\n",
              " 'they',\n",
              " 'change',\n",
              " 'things.',\n",
              " 'They',\n",
              " 'push',\n",
              " 'the',\n",
              " 'human',\n",
              " 'race',\n",
              " 'forward,',\n",
              " 'and',\n",
              " 'while',\n",
              " 'some',\n",
              " 'may',\n",
              " 'see',\n",
              " 'them',\n",
              " 'as',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones,',\n",
              " 'we',\n",
              " 'see',\n",
              " 'genius,',\n",
              " 'because',\n",
              " 'the',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'are',\n",
              " 'crazy',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'think',\n",
              " 'that',\n",
              " 'they',\n",
              " 'can',\n",
              " 'change',\n",
              " 'the',\n",
              " 'world,',\n",
              " 'are',\n",
              " 'the',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'do.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenization with **NLTK**"
      ],
      "metadata": {
        "id": "30AWrW8A7ZkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK stands for Natural Language Toolkit. This is a suite of libraries and programs for statistical natural language processing for English written in Python.\n",
        "\n",
        "NLTK contains a module called ***tokenize*** with a ***word_tokenize()*** method that will help us split a text into tokens. Once you installed NLTK, you can write the following code to tokenize text."
      ],
      "metadata": {
        "id": "u-GNJx0t7qui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It even considers punctuation as a token"
      ],
      "metadata": {
        "id": "Xxfrnm878qS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "44mEG5Du8Hca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a523f8-5850-43d5-8ccd-fad432e120d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # punctuation kit\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokenize(text) # every single non word is split -> ',/ etc"
      ],
      "metadata": {
        "id": "AYm9tEK47dYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23accfc-fb77-4e1c-c60f-aa0f9b84d1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here',\n",
              " '’',\n",
              " 's',\n",
              " 'to',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones',\n",
              " ',',\n",
              " 'the',\n",
              " 'misfits',\n",
              " ',',\n",
              " 'the',\n",
              " 'rebels',\n",
              " ',',\n",
              " 'the',\n",
              " 'troublemakers',\n",
              " ',',\n",
              " 'the',\n",
              " 'round',\n",
              " 'pegs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'square',\n",
              " 'holes',\n",
              " '.',\n",
              " 'The',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'see',\n",
              " 'things',\n",
              " 'differently',\n",
              " '—',\n",
              " 'they',\n",
              " '’',\n",
              " 're',\n",
              " 'not',\n",
              " 'fond',\n",
              " 'of',\n",
              " 'rules',\n",
              " '.',\n",
              " 'You',\n",
              " 'can',\n",
              " 'quote',\n",
              " 'them',\n",
              " ',',\n",
              " 'disagree',\n",
              " 'with',\n",
              " 'them',\n",
              " ',',\n",
              " 'glorify',\n",
              " 'or',\n",
              " 'vilify',\n",
              " 'them',\n",
              " ',',\n",
              " 'but',\n",
              " 'the',\n",
              " 'only',\n",
              " 'thing',\n",
              " 'you',\n",
              " 'can',\n",
              " '’',\n",
              " 't',\n",
              " 'do',\n",
              " 'is',\n",
              " 'ignore',\n",
              " 'them',\n",
              " 'because',\n",
              " 'they',\n",
              " 'change',\n",
              " 'things',\n",
              " '.',\n",
              " 'They',\n",
              " 'push',\n",
              " 'the',\n",
              " 'human',\n",
              " 'race',\n",
              " 'forward',\n",
              " ',',\n",
              " 'and',\n",
              " 'while',\n",
              " 'some',\n",
              " 'may',\n",
              " 'see',\n",
              " 'them',\n",
              " 'as',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones',\n",
              " ',',\n",
              " 'we',\n",
              " 'see',\n",
              " 'genius',\n",
              " ',',\n",
              " 'because',\n",
              " 'the',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'are',\n",
              " 'crazy',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'think',\n",
              " 'that',\n",
              " 'they',\n",
              " 'can',\n",
              " 'change',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'are',\n",
              " 'the',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "qQCDP8nJ8lGE",
        "outputId": "97a6a768-97b7-4b4b-e72c-3915b2fa6af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here’s to the crazy ones, the misfits, the rebels, the troublemakers, \\nthe round pegs in the square holes. The ones who see things differently — they’re not fond of \\nrules. You can quote them, disagree with them, glorify\\nor vilify them, but the only thing you can’t do is ignore them because they\\nchange things. They push the human race forward, and while some may see them\\nas the crazy ones, we see genius, because the ones who are crazy enough to think\\nthat they can change the world, are the ones who do.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sntence tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(text) # sentence is broken at '.'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRYE31pq8hgF",
        "outputId": "6e195b71-8367-450e-8327-32ca24303ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here’s to the crazy ones, the misfits, the rebels, the troublemakers, \\nthe round pegs in the square holes.',\n",
              " 'The ones who see things differently — they’re not fond of \\nrules.',\n",
              " 'You can quote them, disagree with them, glorify\\nor vilify them, but the only thing you can’t do is ignore them because they\\nchange things.',\n",
              " 'They push the human race forward, and while some may see them\\nas the crazy ones, we see genius, because the ones who are crazy enough to think\\nthat they can change the world, are the ones who do.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Tokenize text in different languages with **spaCy**"
      ],
      "metadata": {
        "id": "26znV3BE9ftB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you need to tokenize text written in a language other than English, you can use spaCy. This is a library for advanced natural language processing, written in Python and Cython, that supports tokenization for more than 65 languages.\n",
        "\n",
        "Let’s tokenize the same Steve Jobs text but now translated in Spanish."
      ],
      "metadata": {
        "id": "SIHdLpx59vry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It considers punctuation as a token"
      ],
      "metadata": {
        "id": "y2WjsX3A-CKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.es import Spanish\n",
        "spac = Spanish()\n",
        "\n",
        "text_spanish = \"\"\"Por los locos. Los marginados. Los rebeldes. Los problematicos. \n",
        "Los inadaptados. Los que ven las cosas de una manera distinta. A los que no les gustan\n",
        "las reglas. Y a los que no respetan el “status quo”. Puedes citarlos, discrepar de ellos,\n",
        "ensalzarlos o vilipendiarlos. Pero lo que no puedes hacer es ignorarlos… Porque ellos\n",
        "cambian las cosas, empujan hacia adelante la raza humana y, aunque algunos puedan\n",
        "considerarlos locos, nosotros vemos en ellos a genios. Porque las personas que están\n",
        "lo bastante locas como para creer que pueden cambiar el mundo, son las que lo logran.\"\"\"\n",
        "\n",
        "doc = spac(text_spanish)\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "zMR-5DCN8L36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "107a03e0-e9a4-48a5-e795-31c863efa5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Por', 'los', 'locos', '.', 'Los', 'marginados', '.', 'Los', 'rebeldes', '.', 'Los', 'problematicos', '.', '\\n', 'Los', 'inadaptados', '.', 'Los', 'que', 'ven', 'las', 'cosas', 'de', 'una', 'manera', 'distinta', '.', 'A', 'los', 'que', 'no', 'les', 'gustan', '\\n', 'las', 'reglas', '.', 'Y', 'a', 'los', 'que', 'no', 'respetan', 'el', '“', 'status', 'quo', '”', '.', 'Puedes', 'citarlos', ',', 'discrepar', 'de', 'ellos', ',', '\\n', 'ensalzarlos', 'o', 'vilipendiarlos', '.', 'Pero', 'lo', 'que', 'no', 'puedes', 'hacer', 'es', 'ignorarlos', '…', 'Porque', 'ellos', '\\n', 'cambian', 'las', 'cosas', ',', 'empujan', 'hacia', 'adelante', 'la', 'raza', 'humana', 'y', ',', 'aunque', 'algunos', 'puedan', '\\n', 'considerarlos', 'locos', ',', 'nosotros', 'vemos', 'en', 'ellos', 'a', 'genios', '.', 'Porque', 'las', 'personas', 'que', 'están', '\\n', 'lo', 'bastante', 'locas', 'como', 'para', 'creer', 'que', 'pueden', 'cambiar', 'el', 'mundo', ',', 'son', 'las', 'que', 'lo', 'logran', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "['Por', 'los', 'locos', '.', 'Los', 'marginados', '.', 'Los', 'rebeldes', '.', 'Los', 'problematicos', '.', '\\n', 'Los', 'inadaptados', '.', 'Los', 'que', 'ven', 'las', 'cosas', 'de', 'una', 'manera', 'distinta', '.', 'A', 'los', 'que', 'no', 'les', 'gustan', '\\n', 'las', 'reglas', '.', 'Y', 'a', 'los', 'que', 'no', 'respetan', 'el', '“', 'status', 'quo', '”', '.', 'Puedes', 'citarlos', ',', 'discrepar', 'de', 'ellos', ',', '\\n', 'ensalzarlos', 'o', 'vilipendiarlos', '.', 'Pero', 'lo', 'que', 'no', 'puedes', 'hacer', 'es', 'ignorarlos', '…', 'Porque', 'ellos', '\\n', 'cambian', 'las', 'cosas', ',', 'empujan', 'hacia', 'adelante', 'la', 'raza', 'humana', 'y', ',', 'aunque', 'algunos', 'puedan', '\\n', 'considerarlos', 'locos', ',', 'nosotros', 'vemos', 'en', 'ellos', 'a', 'genios', '.', 'Porque', 'las', 'personas', 'que', 'están', '\\n', 'lo', 'bastante', 'locas', 'como', 'para', 'creer', 'que', 'pueden', 'cambiar', 'el', 'mundo', ',', 'son', 'las', 'que', 'lo', 'logran', '.']\n"
      ],
      "metadata": {
        "id": "YQ7EOlYl1omT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we imported Spanish from spacy.lang.es but if you’re working with text in English, just import English from spacy.lang.en [Check the list of languages available here.](https://spacy.io/usage/models)"
      ],
      "metadata": {
        "id": "7GMITrFp-P3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "e_vjbnzl_LuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Tokenization with **Gensim**\n",
        "\n",
        "Gensim(Generate Similar) is a library for unsupervised topic modeling and natural language processing and also contains a tokenizer. Once you install Gensim, tokenizing text will be as simple as writing the following code.\n",
        "\n",
        "Gensim is quite strict with punctuation. It splits whenever a punctuation is encountered."
      ],
      "metadata": {
        "id": "jCCLzny6-pHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "#word tokenization\n",
        "print(list(tokenize(text)))"
      ],
      "metadata": {
        "id": "5Q17nrr599JJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccdc4f36-8475-4b3d-e9f6-345e8c0f2ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Here', 's', 'to', 'the', 'crazy', 'ones', 'the', 'misfits', 'the', 'rebels', 'the', 'troublemakers', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', 'The', 'ones', 'who', 'see', 'things', 'differently', 'they', 're', 'not', 'fond', 'of', 'rules', 'You', 'can', 'quote', 'them', 'disagree', 'with', 'them', 'glorify', 'or', 'vilify', 'them', 'but', 'the', 'only', 'thing', 'you', 'can', 't', 'do', 'is', 'ignore', 'them', 'because', 'they', 'change', 'things', 'They', 'push', 'the', 'human', 'race', 'forward', 'and', 'while', 'some', 'may', 'see', 'them', 'as', 'the', 'crazy', 'ones', 'we', 'see', 'genius', 'because', 'the', 'ones', 'who', 'are', 'crazy', 'enough', 'to', 'think', 'that', 'they', 'can', 'change', 'the', 'world', 'are', 'the', 'ones', 'who', 'do']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Depending on the use case and requirement, use one of the above"
      ],
      "metadata": {
        "id": "JLv4bnmm_R8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Stop Words**"
      ],
      "metadata": {
        "id": "0Hg1x8isDdev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are a set of commonly used words in a language. Examples of stop words in English are “a”, “the”, “is”, “are” and etc. Stop words are commonly used in Text Mining and NLP to eliminate words that are so commonly used that they carry very little useful information.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "*Why do we remove stop words?*\n",
        "\n",
        "It helps to remove the low-level information from our text in order to give more focus to the important information.\n",
        "Those words do not really contribute significant information to our model.\n",
        "\n",
        "<br>\n",
        "\n",
        "*Do we always remove stop words*\n",
        "\n",
        "Not always. It highly depends on the use case. For example tasks like text classification do not generally need stop words as the other words present in the dataset are more important and give the general idea of the text. So, we generally remove stop words in such tasks.\n",
        "\n",
        "However, in task like sentiment analysis, you might want to maintain these stop words. \n",
        "\n",
        "For example, if we are training a model that can perform the sentiment analysis task, we might not remove the stop words.\n",
        "\n",
        "**Movie review:** *“The movie was not good at all.”*\n",
        "\n",
        "**Text after removal of stop words:** *“movie good”*\n",
        "\n",
        "We can clearly see that the review for the movie was negative. However, after the removal of stop words, the review became positive, which is not the reality. Thus, the removal of stop words can be problematic here."
      ],
      "metadata": {
        "id": "y6ro0kL4DrKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing Stop words with Natural Language Toolkit (NLTK)"
      ],
      "metadata": {
        "id": "vpGKxzYYGbab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords') #first download, default words\n",
        "from nltk.corpus import stopwords #now import"
      ],
      "metadata": {
        "id": "PwB98IavDiQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84fb48ed-dd22-46f4-f100-350e049cc4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw_nltk = stopwords.words('english') # give all such words in english\n",
        "print(sw_nltk)"
      ],
      "metadata": {
        "id": "F8fPr4yuGw4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65634844-6e42-4073-ab7d-1bd69ebbf0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
      ],
      "metadata": {
        "id": "1CJlPoWR5Hd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See all of them in one go:"
      ],
      "metadata": {
        "id": "F9aRd4-zHd-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us check how many stop words this library has."
      ],
      "metadata": {
        "id": "QJbmt3C4H-UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(sw_nltk))"
      ],
      "metadata": {
        "id": "bA-80jJSG-Xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7c48da-1a91-4f6a-e9a2-047e91851f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text2 = \"Here's to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes.\""
      ],
      "metadata": {
        "id": "eiSzoc7TI2yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word for word in text2.split() if word.lower() not in sw_nltk]\n",
        "new_text = \" \".join(words)\n",
        "print(new_text)\n",
        "print(\"Old length: \", len(text2))\n",
        "print(\"New length: \", len(new_text))"
      ],
      "metadata": {
        "id": "_yFtz8PnICK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770aaca6-91c1-402f-d72c-822e1c89e9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's crazy ones, misfits, rebels, troublemakers, round pegs square holes.\n",
            "Old length:  105\n",
            "New length:  75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8emYAfzImAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing Stop words with spaCy"
      ],
      "metadata": {
        "id": "rmGtEznCKhTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "#loading the english language small model of spacy\n",
        "en = spacy.load('en_core_web_sm') #load english stopwords\n",
        "sw_spacy = en.Defaults.stop_words\n",
        "print(sorted((sw_spacy)))"
      ],
      "metadata": {
        "id": "6APClmX9Kl4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3515ae-dba8-4dfa-b7a9-5153bdb6526d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'ca', 'call', 'can', 'cannot', 'could', 'did', 'do', 'does', 'doing', 'done', 'down', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'made', 'make', 'many', 'may', 'me', 'meanwhile', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', \"n't\", 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'n‘t', 'n’t', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'unless', 'until', 'up', 'upon', 'us', 'used', 'using', 'various', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m', '’re', '’s', '’ve']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'ca', 'call', 'can', 'cannot', 'could', 'did', 'do', 'does', 'doing', 'done', 'down', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'made', 'make', 'many', 'may', 'me', 'meanwhile', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', \"n't\", 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'n‘t', 'n’t', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'unless', 'until', 'up', 'upon', 'us', 'used', 'using', 'various', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m', '’re', '’s', '’ve']\n"
      ],
      "metadata": {
        "id": "6Nns_dZt6wCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(sw_spacy))"
      ],
      "metadata": {
        "id": "49H7IVHAKyjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42ed3e3-1496-4dbb-e5da-fdef266dd850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quite a long list of stop words"
      ],
      "metadata": {
        "id": "06vpqCavLJDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word for word in text2.split() if word.lower() not in sw_spacy]\n",
        "new_text = \" \".join(words)\n",
        "print(new_text)\n",
        "print(\"Old length: \", len(text2))\n",
        "print(\"New length: \", len(new_text))"
      ],
      "metadata": {
        "id": "5GnZFB3NLDdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9733d43b-efb8-470f-9214-61931e1111d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's crazy ones, misfits, rebels, troublemakers, round pegs square holes.\n",
            "Old length:  105\n",
            "New length:  75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both NLTK and spaCy did same work in this case. Might not be same in other scenarios."
      ],
      "metadata": {
        "id": "ZRbObZwvLbci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing Stop words with Ginsim"
      ],
      "metadata": {
        "id": "LxSEUckXLsYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
        "print(STOPWORDS)"
      ],
      "metadata": {
        "id": "j_0suPJ-LtGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28344a34-3fdb-4c84-816d-41de7a6f74b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frozenset({'hereupon', 'here', 'would', 'up', 'an', 'can', 'full', 'thereafter', 'third', 'every', 'mill', 'becoming', 'after', 'etc', 'even', 'a', 'own', 'cannot', 'although', 'elsewhere', 'hundred', 'mostly', 'co', 'see', 'hasnt', 'enough', 'this', 'to', 'all', 'without', 'hereby', 'my', 'if', 'empty', 'will', 'anyway', 'under', 'for', 'front', 'become', 'why', 'fire', 'until', 'found', 'herein', 'doing', 'of', 'him', 'sixty', 'might', 'per', 'some', 'did', 'amongst', 'ten', 'system', 'towards', 'hereafter', 'nor', 'toward', 'could', 'almost', 'also', 'been', 'eleven', 'none', 'whoever', 'several', 'either', 'whose', 'herself', 'well', 'nowhere', 'from', 'take', 'they', 'during', 'whither', 'have', 'between', 'just', 'used', 'regarding', 'again', 'unless', 'among', 'because', 'at', 'sincere', 'out', 'else', 'ltd', 'amount', 'meanwhile', 'didn', 'everything', 'hers', 'however', 'done', 'somehow', 'somewhere', 'one', 'show', 'otherwise', 'what', 'last', 'no', 'anyone', 'amoungst', 'being', 'least', 'thin', 'sometime', 'say', 'whence', 'four', 'thereupon', 'very', 'fill', 'beyond', 'please', 'who', 'within', 'me', 'forty', 'hence', 'kg', 'inc', 'anywhere', 'couldnt', 'therein', 'often', 'whom', 'next', 'few', 'made', 'yourself', 'around', 'over', 'side', 'interest', 'wherein', 'the', 'he', 'go', 'by', 'those', 'noone', 'seeming', 'everywhere', 'call', 'whereby', 'beside', 'must', 'via', 'fifteen', 'latterly', 'same', 'latter', 'each', 'whereafter', 'itself', 'describe', 'with', 'alone', 'thence', 'anyhow', 'fify', 'yours', 'various', 'don', 'ourselves', 'sometimes', 'serious', 'his', 'never', 'upon', 'when', 'has', 'such', 'most', 'ever', 'there', 'cant', 'nobody', 'where', 'whereupon', 'moreover', 'con', 'namely', 'top', 'still', 'get', 'computer', 'part', 'whatever', 'themselves', 'may', 'about', 'already', 'keep', 'nine', 'neither', 'you', 'down', 'un', 'below', 'nevertheless', 'now', 'someone', 'is', 'not', 'she', 'further', 'thru', 'wherever', 'it', 'back', 'something', 'than', 'therefore', 'are', 'km', 'were', 'twelve', 'detail', 'becomes', 'whole', 'yourselves', 'above', 'any', 'de', 'off', 'before', 'us', 'beforehand', 'quite', 'these', 'as', 'yet', 'how', 'seems', 'its', 'should', 'eight', 'two', 'formerly', 'eg', 'our', 'seemed', 'perhaps', 'others', 'rather', 'whether', 'does', 'thus', 'be', 'or', 'her', 'less', 'thick', 'am', 'then', 'but', 'put', 'more', 'bill', 'anything', 'onto', 'twenty', 'whereas', 'that', 'find', 'we', 'though', 'himself', 'across', 'really', 'another', 'many', 'too', 'together', 'since', 'i', 'do', 'myself', 'throughout', 'while', 'give', 'everyone', 'indeed', 'doesn', 'always', 'first', 'cry', 'besides', 'nothing', 'into', 'had', 'became', 'both', 'three', 'other', 'former', 'only', 'much', 'thereby', 're', 'five', 'whenever', 'them', 'ours', 'afterwards', 'ie', 'mine', 'seem', 'bottom', 'once', 'in', 'due', 'except', 'using', 'behind', 'against', 'name', 'their', 'and', 'along', 'your', 'on', 'make', 'so', 'move', 'six', 'which', 'was', 'through'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "frozenset({'before', 'yet', 'so', 'him', 'anyone', 'to', 'fifteen', 'then', 'done', 'cry', 'at', 'would', 'two', 'am', 'using', 'ourselves', 'meanwhile', 'onto', 'must', 'while', 'km', 'whenever', 'noone', 'thus', 'fire', 'nevertheless', 'ie', 'computer', 'much', 'part', 'five', 'against', 'doing', 'across', 'system', 'hence', 'give', 'made', 'her', 'is', 'full', 'get', 'off', 'more', 'had', 'itself', 'whether', 'too', 'interest', 'within', 'nobody', 'few', 'six', 'mill', 'thereupon', 'hers', 'once', 'of', 'couldnt', 'became', 'an', 'latterly', 'not', 'however', 'herself', 'be', 'namely', 'almost', 'afterwards', 'should', 'un', 'otherwise', 'eleven', 'seeming', 'less', 'nor', 'again', 'as', 'own', 'me', 'someone', 'con', 'alone', 'others', 'per', 'all', 'nowhere', 'themselves', 'whose', 'next', 'ten', 'my', 'anywhere', 'below', 'bill', 'even', 'up', 'together', 'why', 'call', 'say', 'though', 'a', 'eg', 'one', 'wherein', 'about', 'beside', 'whence', 'hundred', 'thin', 'do', 'twenty', 'into', 'may', 'been', 'seem', 'such', 'behind', 'put', 'really', 'becoming', 'in', 'you', 'it', 'quite', 'formerly', 'hasnt', 'always', 'with', 'mostly', 'both', 'somewhere', 'seems', 'whole', 'among', 'found', 'many', 'along', 'indeed', 'only', 'last', 'go', 'those', 'don', 'each', 'moreover', 'somehow', 'amoungst', 'very', 'often', 'didn', 'any', 'see', 'whatever', 'yours', 'something', 'well', 'down', 'did', 'some', 'three', 'what', 'has', 'back', 'thence', 'move', 'beyond', 'without', 'sincere', 'sometime', 'besides', 'them', 'myself', 'were', 'forty', 'upon', 'enough', 'herein', 'these', 'cant', 'out', 'your', 'whom', 'when', 'rather', 'already', 'third', 'whoever', 'amongst', 'there', 'ours', 'top', 'keep', 'although', 'here', 'side', 'another', 'ever', 'his', 'thereafter', 'several', 'whither', 'except', 'therein', 'via', 'and', 'he', 'please', 'anyhow', 'seemed', 'now', 'does', 'latter', 'find', 'first', 'mine', 'sometimes', 'same', 'whereafter', 'front', 'the', 'will', 'kg', 'around', 'where', 'just', 'on', 'us', 'could', 'yourself', 'still', 'detail', 'fify', 'because', 'thereby', 'nothing', 'perhaps', 'most', 'its', 'describe', 'show', 'through', 'thick', 'every', 'we', 'due', 'beforehand', 'after', 'anyway', 'name', 'himself', 'therefore', 'also', 'whereby', 'from', 'twelve', 'above', 're', 'eight', 'are', 'serious', 'former', 'yourselves', 'nine', 'bottom', 'inc', 'this', 'their', 'under', 'until', 'becomes', 'how', 'used', 'might', 'whereupon', 'by', 'amount', 'she', 'unless', 'but', 'empty', 'throughout', 'four', 'ltd', 'fill', 'have', 'or', 'regarding', 'hereupon', 'if', 'that', 'no', 'our', 'wherever', 'sixty', 'hereafter', 'doesn', 'elsewhere', 'being', 'thru', 'least', 'none', 'various', 'neither', 'between', 'everywhere', 'other', 'co', 'hereby', 'become', 'than', 'was', 'can', 'further', 'for', 'towards', 'etc', 'they', 'everything', 'since', 'make', 'over', 'whereas', 'who', 'else', 'which', 'cannot', 'either', 'de', 'never', 'everyone', 'take', 'during', 'anything', 'toward', 'i'})\n"
      ],
      "metadata": {
        "id": "_ULK-xAu7iVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(STOPWORDS))"
      ],
      "metadata": {
        "id": "qBPCoMVhLyrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0948f999-04cf-4496-bc73-c75a82725c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = remove_stopwords(text2)\n",
        "print(new_text)\n",
        "print(\"Old length: \", len(text2))\n",
        "print(\"New length: \", len(new_text))"
      ],
      "metadata": {
        "id": "XpDk2niKMqqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f557382-1fbf-4878-e794-cd1a6b06caec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's crazy ones, misfits, rebels, troublemakers, round pegs square holes.\n",
            "Old length:  105\n",
            "New length:  75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing Stop words with Scikit-Learn"
      ],
      "metadata": {
        "id": "Kkf0WXo9M94y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "print(ENGLISH_STOP_WORDS)"
      ],
      "metadata": {
        "id": "tAEICRJrNBt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ea0670-4d4e-43df-cf46-4bfc38f2da1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frozenset({'hereupon', 'here', 'would', 'an', 'up', 'can', 'full', 'thereafter', 'third', 'every', 'mill', 'becoming', 'after', 'etc', 'even', 'a', 'own', 'cannot', 'although', 'elsewhere', 'hundred', 'mostly', 'co', 'see', 'hasnt', 'enough', 'this', 'to', 'all', 'without', 'hereby', 'my', 'if', 'empty', 'will', 'anyway', 'under', 'for', 'front', 'why', 'become', 'fire', 'herein', 'found', 'until', 'of', 'him', 'sixty', 'might', 'per', 'some', 'amongst', 'ten', 'system', 'towards', 'hereafter', 'nor', 'toward', 'could', 'almost', 'also', 'been', 'eleven', 'whoever', 'none', 'several', 'either', 'whose', 'herself', 'well', 'nowhere', 'from', 'take', 'they', 'during', 'have', 'whither', 'between', 'again', 'among', 'at', 'because', 'sincere', 'out', 'else', 'ltd', 'amount', 'meanwhile', 'everything', 'hers', 'however', 'done', 'somehow', 'somewhere', 'one', 'show', 'otherwise', 'what', 'last', 'no', 'anyone', 'amoungst', 'being', 'least', 'thin', 'sometime', 'whence', 'four', 'thereupon', 'very', 'fill', 'beyond', 'please', 'who', 'within', 'me', 'forty', 'hence', 'inc', 'anywhere', 'couldnt', 'therein', 'often', 'whom', 'next', 'few', 'made', 'yourself', 'around', 'over', 'side', 'interest', 'the', 'wherein', 'he', 'go', 'by', 'those', 'noone', 'seeming', 'everywhere', 'call', 'whereby', 'beside', 'must', 'via', 'fifteen', 'latterly', 'same', 'latter', 'each', 'whereafter', 'itself', 'describe', 'with', 'alone', 'thence', 'anyhow', 'yours', 'ourselves', 'sometimes', 'serious', 'his', 'never', 'upon', 'when', 'has', 'such', 'most', 'ever', 'there', 'cant', 'nobody', 'where', 'whereupon', 'moreover', 'con', 'namely', 'top', 'still', 'get', 'part', 'whatever', 'may', 'themselves', 'about', 'already', 'keep', 'nine', 'neither', 'you', 'down', 'un', 'below', 'nevertheless', 'now', 'someone', 'is', 'not', 'she', 'further', 'thru', 'wherever', 'it', 'back', 'something', 'than', 'therefore', 'are', 'were', 'twelve', 'detail', 'becomes', 'whole', 'yourselves', 'above', 'any', 'de', 'off', 'before', 'us', 'beforehand', 'these', 'as', 'yet', 'how', 'seems', 'its', 'eight', 'should', 'two', 'formerly', 'eg', 'our', 'seemed', 'perhaps', 'others', 'rather', 'whether', 'thus', 'be', 'or', 'her', 'less', 'thick', 'am', 'then', 'but', 'put', 'more', 'bill', 'anything', 'onto', 'twenty', 'whereas', 'that', 'find', 'we', 'though', 'himself', 'across', 'another', 'many', 'too', 'together', 'since', 'i', 'do', 'myself', 'throughout', 'while', 'give', 'everyone', 'indeed', 'always', 'first', 'cry', 'besides', 'nothing', 'into', 'had', 'became', 'both', 'other', 'three', 'former', 'much', 'only', 'thereby', 'five', 're', 'whenever', 'them', 'afterwards', 'ours', 'ie', 'mine', 'seem', 'bottom', 'once', 'in', 'due', 'except', 'behind', 'against', 'name', 'their', 'and', 'along', 'your', 'on', 'so', 'move', 'six', 'which', 'was', 'through', 'fifty'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "frozenset({'before', 'yet', 'so', 'him', 'anyone', 'to', 'fifteen', 'then', 'done', 'cry', 'at', 'would', 'two', 'am', 'ourselves', 'meanwhile', 'onto', 'must', 'while', 'whenever', 'noone', 'thus', 'fire', 'nevertheless', 'ie', 'much', 'five', 'part', 'against', 'across', 'system', 'give', 'hence', 'made', 'her', 'is', 'full', 'get', 'off', 'more', 'had', 'itself', 'whether', 'too', 'interest', 'within', 'nobody', 'few', 'six', 'hers', 'thereupon', 'mill', 'once', 'of', 'couldnt', 'an', 'became', 'latterly', 'not', 'however', 'herself', 'be', 'namely', 'almost', 'afterwards', 'should', 'un', 'otherwise', 'eleven', 'seeming', 'less', 'nor', 'again', 'as', 'own', 'me', 'con', 'someone', 'alone', 'others', 'per', 'all', 'nowhere', 'themselves', 'whose', 'next', 'ten', 'my', 'anywhere', 'below', 'bill', 'even', 'up', 'together', 'why', 'call', 'though', 'a', 'eg', 'one', 'wherein', 'about', 'beside', 'whence', 'hundred', 'thin', 'do', 'twenty', 'into', 'may', 'been', 'seem', 'such', 'behind', 'put', 'becoming', 'in', 'you', 'it', 'formerly', 'hasnt', 'always', 'with', 'mostly', 'both', 'somewhere', 'seems', 'whole', 'among', 'found', 'many', 'along', 'fifty', 'indeed', 'only', 'last', 'those', 'go', 'each', 'moreover', 'somehow', 'amoungst', 'often', 'very', 'any', 'see', 'whatever', 'yours', 'something', 'well', 'down', 'some', 'three', 'what', 'has', 'back', 'thence', 'move', 'beyond', 'without', 'sincere', 'sometime', 'besides', 'them', 'myself', 'were', 'forty', 'upon', 'enough', 'herein', 'these', 'cant', 'out', 'your', 'whom', 'when', 'rather', 'already', 'third', 'whoever', 'amongst', 'there', 'ours', 'top', 'keep', 'although', 'here', 'side', 'another', 'ever', 'his', 'several', 'except', 'thereafter', 'whither', 'therein', 'via', 'and', 'he', 'please', 'anyhow', 'seemed', 'now', 'latter', 'find', 'first', 'mine', 'sometimes', 'same', 'whereafter', 'front', 'the', 'will', 'around', 'where', 'on', 'us', 'could', 'yourself', 'still', 'detail', 'because', 'thereby', 'nothing', 'perhaps', 'most', 'its', 'describe', 'show', 'through', 'thick', 'every', 'we', 'due', 'beforehand', 'after', 'anyway', 'name', 'himself', 'therefore', 'also', 'whereby', 'from', 'twelve', 'above', 're', 'eight', 'are', 'serious', 'former', 'yourselves', 'nine', 'bottom', 'inc', 'this', 'their', 'under', 'until', 'becomes', 'how', 'might', 'whereupon', 'by', 'amount', 'she', 'but', 'empty', 'throughout', 'four', 'ltd', 'fill', 'have', 'or', 'hereupon', 'if', 'that', 'no', 'our', 'wherever', 'sixty', 'hereafter', 'elsewhere', 'being', 'thru', 'least', 'none', 'neither', 'between', 'other', 'everywhere', 'co', 'hereby', 'become', 'than', 'was', 'can', 'further', 'for', 'towards', 'etc', 'everything', 'they', 'since', 'over', 'whereas', 'who', 'else', 'which', 'cannot', 'either', 'de', 'never', 'everyone', 'take', 'anything', 'during', 'toward', 'i'})\n"
      ],
      "metadata": {
        "id": "AjicoFh68JGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(ENGLISH_STOP_WORDS))"
      ],
      "metadata": {
        "id": "i8jL_5vhN-dD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8792467-3490-4d72-bee6-7f118f65047b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word for word in text2.split() if word.lower() not in ENGLISH_STOP_WORDS]\n",
        "new_text = \" \".join(words)\n",
        "print(new_text)\n",
        "print(\"Old length: \", len(text2))\n",
        "print(\"New length: \", len(new_text))"
      ],
      "metadata": {
        "id": "4hxMHBSnOHPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6a205b-5a4d-45fc-d2c6-71aecd094732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's crazy ones, misfits, rebels, troublemakers, round pegs square holes.\n",
            "Old length:  105\n",
            "New length:  75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UTan84n8ONCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding custom Stop Words"
      ],
      "metadata": {
        "id": "oiry74pEOXu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also add custom stop words to the list of stop words available in these libraries to serve our purpose.\n",
        "\n",
        "Here is the code to add some custom stop words to NLTK’s stop words list:"
      ],
      "metadata": {
        "id": "PK1ZqNWYOUxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sw_nltk.extend(['first', 'second', 'third', 'me'])\n",
        "print(len(sw_nltk))"
      ],
      "metadata": {
        "id": "_KCRhgifOWmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398a8ed5-0d62-4e83-abce-712bae418268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing Stop Words"
      ],
      "metadata": {
        "id": "w2Y8Ova2OsDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also remove stop words from the list available in these libraries.\n",
        "\n",
        "Here is the code using the NLTK library:"
      ],
      "metadata": {
        "id": "rZ5RjAc2Ophx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sw_nltk.remove('not')\n",
        "print(len(sw_nltk))"
      ],
      "metadata": {
        "id": "_1UdorNtOgEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17737c6a-a79a-4ada-bea8-4c8d7866c577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FuBLmQ52OzOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Custom Stop Words"
      ],
      "metadata": {
        "id": "K75CDeUuPQzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text2 = \"Here's to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes.\""
      ],
      "metadata": {
        "id": "uBwj5TaG9QWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create your custom stop words list\n",
        "my_stop_words = ['to','the','in']\n",
        "words = [word for word in text2.split() if word.lower() not in my_stop_words]\n",
        "new_text = \" \".join(words)\n",
        "print(new_text)\n",
        "print(\"Old length: \", len(text2))\n",
        "print(\"New length: \", len(new_text))"
      ],
      "metadata": {
        "id": "iD-JVtgAPUAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90d5ec8-a895-44c0-b041-787c006d3e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's crazy ones, misfits, rebels, troublemakers, round pegs square holes.\n",
            "Old length:  105\n",
            "New length:  75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IpO-BbcrPWz4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
